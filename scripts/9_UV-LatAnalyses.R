# Analysis scripts for Baez-Ortega et al., 2019
# Step 9. Association analyses of signature 7 exposure, CC>TT mutations and latitude

# Adrian Baez-Ortega, 2019


# TO RUN THIS SCRIPT IN THE TERMINAL
# ----------------------------------
# Run the commands below in the terminal, replacing '/path/to/TCG2019' with
# the path to the TCG2019 directory.
#
#    cd /path/to/TCG2019
#    Rscript scripts/9_UV-LatAnalyses.R


# TO RUN THIS SCRIPT IN RSTUDIO
# -----------------------------
# Before starting, run the line below in RStudio, replacing '/path/to/TCG2019' with
# the path to the TCG2019 directory.
#
#    setwd("path/to/TCG2019")


# If the paths to the input or output files differ from the ones
# used in this script, they may be updated by modifying the lines below.

INPUT = list(
    
    # Path to input file of tumour latitudes
    LATITUDE = file.path("data", "original", "Sample_Latitude.tsv"),
    
    # Path to input file generated by script 2_ImportVariants.R
    VAR.TABLES = file.path("data", "processed", "Variant_Tables.RData"),
    
    # Path to input file generated by script 5_PhyloGroups.R
    PHYLO.GROUPS = file.path("data", "processed", "Phylo_Tree_Groups.RData"),
    
    # Path to input file generated by script 7_SignatureAnalyses.R
    SIGNATURES = file.path("data", "processed", "Signatures_Exposures.RData"),
    
    # Path to input file generated by script 8_DinucAnalyses.R
    DINUCLEOTIDES = file.path("data", "processed", "Dinucleotides.RData")
    
)

OUTPUT = list(
    
    # Path to output PDFs
    S7.REGRESSION = file.path("output", "UV_Regression_CC>TT_Sig7.pdf"),
    LAT.REGRESSION = file.path("output", "UV_Regression_CC>TT_Latitude.pdf"),
    
    # Path to output text summaries
    S7.SUMMARY = file.path("output", "UV_Regression_Stats_CC>TT_Sig7.txt"),
    LAT.SUMMARY = file.path("output", "UV_Regression_Stats_CC>TT_Latitude.txt"),
    
    # Path to output group data table
    GROUP.DATA = file.path("output", "Phylo_Groups_Data.tsv")
    
)


# Print input and output file paths
cat("\nInput files:")
for (name in INPUT) {
    cat("\n  ", name)
}
cat("\n\nOutput files:")
for (name in OUTPUT) {
    cat("\n  ", name)
}
cat("\n\n")


# Load packages
PACKAGES = c("coda", "rstan", "stringr")
cat("Loading packages:", paste(PACKAGES, collapse=", "), "\n")
for (package in PACKAGES) {
    suppressWarnings(suppressMessages(library(package, character.only=TRUE)))
}


# Load input data
cat("Loading data...\n")
tum.latitude = read.table(INPUT$LATITUDE, sep="\t", header=T, stringsAsFactors=F)
load(INPUT$VAR.TABLES)
load(INPUT$PHYLO.GROUPS)
load(INPUT$SIGNATURES)
load(INPUT$DINUCLEOTIDES)


## (1) Association between signature 7 exposure and CC>TT dinucleotides
cat("\nAssociation between signature 7 exposure and CC>TT mutations:\n")

# Obtain counts of group-unique variants, group-unique CC>TT and group-unique [C>T]pG
group.data = exposures.final
group.data$`Group ID` = group.ids
group.data$`Group label` = group.names
group.data$`Number of tumours` = colSums(phylo.groups.idx)
group.data$`Group-unique SNVs` = rowSums(group.spectra)
group.data$`Group-unique [C>T]pG` = apply(group.unique.idx, 2, function(grp.idx) {
    vars = snvs.metadata[tumour.only.idx, ][grp.idx, ]
    ctx = substr(str_split_fixed(vars$INFO, ";", 20)[,12], 13, 15)
    stopifnot(identical(as.character(vars$REF), substr(ctx, 2, 2)))
    sum((vars$REF == "C" & vars$ALT == "T" & substr(ctx, 2, 3) == "CG") |
            (vars$REF == "G" & vars$ALT == "A" & substr(ctx, 1, 2) == "CG"))
})
group.data$`Group-unique CC>TT` = group.unique.cctt
group.data$`Normalised group-unique CC>TT` = 
    group.data$`Group-unique CC>TT` / group.data$`Group-unique [C>T]pG`

# (a) Estimate Pearson's correlation with robust Bayesian model
cat("Estimating robust Bayesian Pearson's correlation...\n")
cor.model = stan.model = "
    data {
        int<lower=1> N;  // number of observations
        vector[2] x[N];  // input data: rows are observations, columns are the two variables
    }
    parameters {
        vector[2] mu;                 // locations of the marginal t distributions
        real<lower=0> sigma[2];       // scales of the marginal t distributions
        real<lower=1> nu;             // degrees of freedom of the marginal t distributions
        real<lower=-1, upper=1> rho;  // correlation coefficient
    }
    transformed parameters {
        // Covariance matrix
        cov_matrix[2] cov = [[       sigma[1] ^ 2       , sigma[1] * sigma[2] * rho ],
                             [ sigma[1] * sigma[2] * rho,       sigma[2] ^ 2        ]];
    }
    model {
        // Likelihood
        // Bivariate Student's t distribution instead of normal for robustness
        x ~ multi_student_t(nu, mu, cov);
        // Noninformative priors on all parameters
        sigma ~ normal(0, 1000);
        mu ~ normal(0, 1000);
        nu ~ gamma(2, 0.1);
    }
    generated quantities {
        // Random samples from the estimated bivariate t distribution (for assessment of fit)
        vector[2] x_rand;
        x_rand = multi_student_t_rng(nu, mu, cov);
    }"

# Exclude groups with <700 unique variants (groups 26, 39, 47, 55)
idx = group.data$`Group-unique SNVs` >= 700
cor.model.data = list(N=sum(idx),
                      x=cbind(group.data$`Group-unique CC>TT`, 
                              group.data$`Signature 7 exposure (mean)` * 
                                  group.data$`Group-unique SNVs`)[idx, ])

# Run MCMC sampling
sink("/dev/null")
cor.stan = stan(model_name="robust_correlation",
                model_code=cor.model, data=cor.model.data,
                iter=7000, warmup=4000, chains=3, seed=911)
sink()

# Retrieve MCMC samples of rho
rho.samples = extract(cor.stan, "rho")[[1]]
rho.hpd95 = HPDinterval(as.mcmc(as.numeric(rho.samples)), prob=0.95)
rho.hpd99 = HPDinterval(as.mcmc(as.numeric(rho.samples)), prob=0.99)

## Plot trace and posterior
# stan_trace(cor.stan, pars="rho")
# stan_dens(cor.stan, pars="rho")

# Print statistics
cat("ASSOCIATION BETWEEN SIGNATURE 7 EXPOSURE AND CC>TT MUTATIONS\n\n",
    "PEARSON'S RHO: POSTERIOR STATISTICS\n",
    "Posterior mean and standard deviation:     Mean = ",
    mean(rho.samples), ", SD = ", sd(rho.samples), "\n",
    "Posterior median and MAD:                  Median = ",
    median(rho.samples), ", MAD = ", mad(rho.samples), "\n",
    "Rho values with 99% posterior probability: 99% HPDI = [", 
    rho.hpd99[,"lower"], ", ", rho.hpd99[,"upper"], "]\n",
    "Rho values with 95% posterior probability: 95% HPDI = [", 
    rho.hpd95[,"lower"], ", ", rho.hpd95[,"upper"], "]\n",
    "Posterior probability that rho is ≤0:      P(rho ≤ 0) = ", 
    mean(rho.samples <= 0), "\n",
    "Posterior probability that rho is ≥0:      P(rho ≥ 0) = ", 
    mean(rho.samples >= 0), "\n",
    "Posterior probability that rho is weak:    P(-0.1 < rho < 0.1) = ", 
    mean(rho.samples > -0.1 & rho.samples < 0.1), "\n\n",
    sep="", file=OUTPUT$S7.SUMMARY)


# (b) Estimate linear regression with robust Bayesian model
cat("Estimating robust Bayesian linear regression...\n")
reg.model = "
    data {
        int<lower=1> N;    // number of observations
        int<lower=0> M;    // number of values for credible interval estimation
        int<lower=0> P;    // number of values to predict
        vector[N] x;       // input data for the explanatory (independent) variable
        vector[N] y;       // input data for the response (dependent) variable
        vector[M] x_cred;  // x values for credible interval estimation (should cover range of x)
        vector[P] x_pred;  // x values for prediction
    }
    parameters {
        real alpha;           // intercept
        real beta;            // coefficient
        real<lower=0> sigma;  // scale of the t distribution
        real<lower=1> nu;     // degrees of freedom of the t distribution
    }
    transformed parameters {
        vector[N] mu = alpha + beta * x;            // mean response
        vector[M] mu_cred = alpha + beta * x_cred;  // mean response for credible interval estimation
        vector[P] mu_pred = alpha + beta * x_pred;  // mean response for prediction
    }
    model {
        // Likelihood
        // Student's t distribution instead of normal for robustness
        y ~ student_t(nu, mu, sigma);
        // Uninformative priors on all parameters
        alpha ~ normal(0, 1000);
        beta ~ normal(0, 1000);
        sigma ~ normal(0, 1000);
        nu ~ gamma(2, 0.1);
    }
    generated quantities {
        // Sample from the t distribution at the values to predict (for prediction)
        real y_pred[P];
        for (p in 1:P) {
            y_pred[p] = student_t_rng(nu, mu_pred[p], sigma);
        }
    }"

# Exclude groups with <700 unique variants (groups 26, 39, 47, 55)
A1.idx = group.data$`Group ID` == "A1"
idx = group.data$`Group-unique SNVs` >= 700
reg.model.data = list(N=sum(idx), M=20, P=1,
                      x=group.data$`Group-unique CC>TT`[idx], 
                      y=(group.data$`Signature 7 exposure (mean)` * 
                             group.data$`Group-unique SNVs`)[idx], 
                      x_pred=as.array(group.data$`Group-unique CC>TT`[A1.idx]),
                      x_cred=seq(min(group.data$`Group-unique CC>TT`), 
                                 max(group.data$`Group-unique CC>TT`), length.out=20))

# Run MCMC sampling
sink("/dev/null")
reg.stan = stan(model_name="robust_regression",
                model_code=reg.model, data=reg.model.data, 
                iter=10000, warmup=5000, chains=3, seed=911)
sink()

## Plot trace and posterior for beta (slope)
#stan_trace(reg.stan, pars="beta")
#stan_dens(reg.stan, pars="beta")

# Extract MCMC samples
alpha.samples = extract(reg.stan, "alpha")[[1]]
beta.samples = extract(reg.stan, "beta")[[1]]
mu.cred.samples = extract(reg.stan, "mu_cred")[[1]]
y.pred.samples = extract(reg.stan, "y_pred")[[1]]

# Obtain HPD intervals
alpha.hpd = list("95"=NULL, "99"=NULL)
beta.hpd = list("95"=NULL, "99"=NULL)
alpha.hpd$`95` = HPDinterval(as.mcmc(as.numeric(alpha.samples)), prob=0.95)
alpha.hpd$`99` = HPDinterval(as.mcmc(as.numeric(alpha.samples)), prob=0.99)
beta.hpd$`95` = HPDinterval(as.mcmc(as.numeric(beta.samples)), prob=0.95)
beta.hpd$`99` = HPDinterval(as.mcmc(as.numeric(beta.samples)), prob=0.99)
y.pred.hpd = list("90"=NULL, "95"=NULL, "99"=NULL)
y.pred.hpd$`90` = apply(y.pred.samples, 2, function(y) HPDinterval(as.mcmc(as.numeric(y)), prob=0.90))
y.pred.hpd$`95` = apply(y.pred.samples, 2, function(y) HPDinterval(as.mcmc(as.numeric(y)), prob=0.95))
y.pred.hpd$`99` = apply(y.pred.samples, 2, function(y) HPDinterval(as.mcmc(as.numeric(y)), prob=0.99))
y.pred.median = apply(y.pred.samples, 2, median)
y.pred.orig = (group.data$`Signature 7 exposure (mean)` * group.data$`Group-unique SNVs`)[A1.idx]
mu.hpd95 = list(upper=NULL, lower=NULL)
for (j in 1:reg.model.data$M) {
    hpd = HPDinterval(as.mcmc(as.numeric(mu.cred.samples[, j])), prob=0.95)
    mu.hpd95$upper = c(mu.hpd95$upper, hpd[, "upper"])
    mu.hpd95$lower = c(mu.hpd95$lower, hpd[, "lower"])
}

# Print statistics
cat("SIMPLE LINEAR REGRESSION: POSTERIOR STATISTICS\n",
    "Intercept (alpha)\n",
    "   Posterior mean and standard deviation: Mean = ", 
    mean(alpha.samples), ", SD = ", sd(alpha.samples), "\n",
    "   Posterior median and MAD:              Median = ", 
    median(alpha.samples), ", MAD = ", mad(alpha.samples), "\n",
    "   Values with 95% posterior probability: 95% HPDI = [", 
    alpha.hpd$`95`[,"lower"], ", ", alpha.hpd$`95`[,"upper"], "]\n",
    "   Values with 99% posterior probability: 99% HPDI = [", 
    alpha.hpd$`99`[,"lower"], ", ", alpha.hpd$`99`[,"upper"], "]\n",
    "Slope (beta)\n",
    "   Posterior mean and standard deviation: Mean = ",
    mean(beta.samples), ", SD = ", sd(beta.samples), "\n",
    "   Posterior median and MAD:              Median = ", 
    median(beta.samples), ", MAD = ", mad(beta.samples), "\n",
    "   Values with 95% posterior probability: 95% HPDI = [", 
    beta.hpd$`95`[,"lower"], ", ", beta.hpd$`95`[,"upper"], "]\n",
    "   Values with 99% posterior probability: 99% HPDI = [", 
    beta.hpd$`99`[,"lower"], ", ", beta.hpd$`99`[,"upper"], "]\n",
    sep="", file=OUTPUT$S7.SUMMARY, append=TRUE)
for (j in 1:reg.model.data$P) {
    cat("Predicted value #", j, " (x = ", reg.model.data$x_pred[j], ")\n",
        "   Posterior median predicted response:   Median = ", y.pred.median[j], "\n",
        "   Values with 90% posterior probability: 90% HPDI = [", 
        y.pred.hpd$`90`[1, j], ", ", y.pred.hpd$`90`[2, j], "]\n",
        "   Values with 95% posterior probability: 95% HPDI = [", 
        y.pred.hpd$`95`[1, j], ", ", y.pred.hpd$`95`[2, j], "]\n",
        "   Values with 99% posterior probability: 99% HPDI = [", 
        y.pred.hpd$`99`[1, j], ", ", y.pred.hpd$`99`[2, j], "]\n",
        sep="", file=OUTPUT$S7.SUMMARY, append=TRUE)
}

# Plot regression
cairo_pdf(OUTPUT$S7.REGRESSION, width=9, height=7)
par(mar=c(4, 5.5, 1, 1), mgp=c(3, 0.8, 0))
plot(1, type="n", ylab="", xlab="", las=1, cex.axis=1.2,
     xlim=c(min(reg.model.data$x) - 0.001*abs(min(reg.model.data$x)), 
            max(reg.model.data$x) + 0.01*abs(max(reg.model.data$x))),
     ylim=c(min(reg.model.data$y) - 0.01*abs(min(reg.model.data$y)), 
            max(reg.model.data$y) + 0.015*abs(max(reg.model.data$y))))
polygon(x=c(reg.model.data$x_cred, rev(reg.model.data$x_cred)), 
        y=c(mu.hpd95$lower, rev(mu.hpd95$upper)), border=NA, col="grey85")
segments(x0=min(reg.model.data$x), 
         y0=median(alpha.samples) + median(beta.samples) * min(reg.model.data$x), 
         x1=max(reg.model.data$x), 
         y1=median(alpha.samples) + median(beta.samples) * max(reg.model.data$x), 
         lwd=3, col="grey30")
points(reg.model.data$x, reg.model.data$y, pch=16, col="dodgerblue3", cex=1.1)
mtext("CC>TT dinucleotide variants", side=1, line=2.5, cex=1.3)
mtext("Signature 7 exposure (SNVs)", side=2, line=4, cex=1.3)
segments(x0=reg.model.data$x_pred, y0=y.pred.hpd$`90`[1,], 
         x1=reg.model.data$x_pred, y1=y.pred.hpd$`90`[2,], 
         col="darkorange2", lwd=3)
points(x=rep(reg.model.data$x_pred, 2), y=c(colMeans(y.pred.samples), y.pred.orig),
       pch=c(23, 16), bg="grey30", cex=1.5, col="darkorange2")
text(x=rep(reg.model.data$x_pred * 0.98, 2), 
     y=c(y.pred.orig * 0.997, colMeans(y.pred.samples) * 1.25),
     labels=paste0("Basal trunk ancestral variation (A1)", 
                   c("\nInferred signature 7 exposure", "\nPredicted signature 7 exposure")),
     col="darkorange2", adj=1, cex=1.2)
mtext(paste0("ρ = ", round(median(rho.samples), 2)),
      side=3, line=-3.5, adj=0, at=0, cex=1.3)
mtext(paste0("95% HPDI = [", round(rho.hpd95[1], 2), ", ", round(rho.hpd95[2], 2), "]"),
      side=3, line=-5, adj=0, at=0, cex=1.3)
mtext(paste0("Slope = ", round(median(beta.samples), 2)),
      side=3, line=-9, adj=0, at=0, cex=1.3)
mtext(paste0("95% HPDI = [", round(beta.hpd$`95`[1], 2), ", ", round(beta.hpd$`95`[2], 2), "]"),
      side=3, line=-10.5, adj=0, at=0, cex=1.3)
invisible(dev.off())



## (2) Association between CC>TT dinucleotides and mean absolute latitude
cat("\nAssociation between normalised CC>TT mutations and mean absolute latitude:\n")

# We only consider phylogenetic groups with >700 unique variants, and in
# which all tumours were collected in the same country or geographic area 
# (i.e. we exclude groups 1, 26, 36, 39, 41, 42, 43, 45, 46, 47, 50, 53, 55, A1, A2, A3)
stopifnot(identical(tum.latitude$Tumour, samples[tumours.tree]))
excluded.ids = c(1, 26, 36, 39, 41, 42, 43, 45, 46, 47, 50, 53, 55, "A1", "A2", "A3")
lat.idx = !(group.data$`Group ID` %in% excluded.ids)
cat("Groups excluded from UV-latitude analysis:", paste(excluded.ids, collapse=", "), "\n")

# Obtain group mean absolute latitudes
stopifnot(identical(group.data$`Group label`, colnames(phylo.groups.idx)))
group.data$`Absolute mean latitude` = apply(phylo.groups.idx, 2, function(grp) {
    abs(mean(tum.latitude$Latitude[grp]))
})
group.data$`Absolute mean latitude`[!lat.idx] = NA


# (a) Estimate Spearman's correlation with robust Bayesian model
cat("Estimating robust Bayesian Spearman's correlation...\n")

# Spearman's correlation is the Pearson's correlation between the variables' ranks
cor.model.data = list(N=sum(lat.idx),
                      x=cbind(rank(group.data$`Normalised group-unique CC>TT`[lat.idx]), 
                              rank(group.data$`Absolute mean latitude`[lat.idx])))

# Run MCMC sampling
sink("/dev/null")
cor.stan = stan(model_name="robust_correlation",
                model_code=cor.model, data=cor.model.data,
                iter=7000, warmup=4000, chains=3, seed=911)
sink()

# Retrieve MCMC samples of rho
rho.samples = extract(cor.stan, "rho")[[1]]
rho.hpd95 = HPDinterval(as.mcmc(as.numeric(rho.samples)), prob=0.95)
rho.hpd99 = HPDinterval(as.mcmc(as.numeric(rho.samples)), prob=0.99)

## Plot trace and posterior
# stan_trace(cor.stan, pars="rho")
# stan_dens(cor.stan, pars="rho")

# Print statistics
cat("ASSOCIATION BETWEEN NORMALISED CC>TT MUTATIONS AND ABSOLUTE MEAN LATITUDE\n\n",
    "SPEARMAN'S RHO: POSTERIOR STATISTICS\n",
    "Posterior mean and standard deviation:     Mean = ",
    mean(rho.samples), ", SD = ", sd(rho.samples), "\n",
    "Posterior median and MAD:                  Median = ",
    median(rho.samples), ", MAD = ", mad(rho.samples), "\n",
    "Rho values with 99% posterior probability: 99% HPDI = [", 
    rho.hpd99[,"lower"], ", ", rho.hpd99[,"upper"], "]\n",
    "Rho values with 95% posterior probability: 95% HPDI = [", 
    rho.hpd95[,"lower"], ", ", rho.hpd95[,"upper"], "]\n",
    "Posterior probability that rho is ≤0:      P(rho ≤ 0) = ", 
    mean(rho.samples <= 0), "\n",
    "Posterior probability that rho is ≥0:      P(rho ≥ 0) = ", 
    mean(rho.samples >= 0), "\n",
    "Posterior probability that rho is weak:    P(-0.1 < rho < 0.1) = ", 
    mean(rho.samples > -0.1 & rho.samples < 0.1), "\n\n",
    sep="", file=OUTPUT$LAT.SUMMARY)


# (b) Estimate logarithmic regression with robust Bayesian model
cat("Estimating robust Bayesian logarithmic regression...\n")
log.reg.model = "
    data {
        int<lower=1> N;    // number of observations
        int<lower=0> M;    // number of values for credible interval estimation
        int<lower=0> P;    // number of values to predict
        vector[N] x;       // input data for the explanatory (independent) variable
        vector[N] y;       // input data for the response (dependent) variable
        vector[M] x_cred;  // x values for credible interval estimation (should cover range of x)
        vector[P] x_pred;  // x values for prediction
    }
    parameters {
        real alpha;           // intercept
        real beta;            // coefficient
        real<lower=0> sigma;  // scale of the t distribution
        real<lower=1> nu;     // degrees of freedom of the t distribution
    }
    transformed parameters {
        vector[N] mu = alpha + beta * log(x);            // mean response (log regression)
        vector[M] mu_cred = alpha + beta * log(x_cred);  // mean response for credible int. estimation
        vector[P] mu_pred = alpha + beta * log(x_pred);  // mean response for prediction
    }
    model {
        // Likelihood
        // Student's t distribution instead of normal for robustness
        y ~ student_t(nu, mu, sigma);
        // Uninformative priors on all parameters
        alpha ~ normal(0, 1000);
        beta ~ normal(0, 1000);
        sigma ~ normal(0, 1000);
        nu ~ gamma(2, 0.1);
    }
    generated quantities {
        // Sample from the t distribution at the values to predict (for prediction)
        real y_pred[P];
        for (p in 1:P) {
            y_pred[p] = student_t_rng(nu, mu_pred[p], sigma);
        }
    }"

# We normalise number of CC>TT mutations by number of [C>T]pG mutations
reg.model.data = list(N=sum(lat.idx), M=50, P=1,
                      x=group.data$`Normalised group-unique CC>TT`[lat.idx], 
                      y=group.data$`Absolute mean latitude`[lat.idx], 
                      x_pred=as.array(group.data$`Normalised group-unique CC>TT`[A1.idx]),
                      x_cred=seq(min(group.data$`Normalised group-unique CC>TT`[lat.idx]), 
                                 max(group.data$`Normalised group-unique CC>TT`[lat.idx]), 
                                 length.out=50))

# Run MCMC sampling
sink("/dev/null")
reg.stan = stan(model_name="robust_log_regression",
                model_code=log.reg.model, data=reg.model.data,
                iter=15000, warmup=5000, chains=4, seed=911)
sink()

# Extract MCMC samples
alpha.samples = extract(reg.stan, "alpha")[[1]]
beta.samples = extract(reg.stan, "beta")[[1]]
mu.cred.samples = extract(reg.stan, "mu_cred")[[1]]
y.pred.samples = extract(reg.stan, "y_pred")[[1]]

# Obtain HPD intervals
alpha.hpd = list("95"=NULL, "99"=NULL)
beta.hpd = list("95"=NULL, "99"=NULL)
alpha.hpd$`95` = HPDinterval(as.mcmc(as.numeric(alpha.samples)), prob=0.95)
alpha.hpd$`99` = HPDinterval(as.mcmc(as.numeric(alpha.samples)), prob=0.99)
beta.hpd$`95` = HPDinterval(as.mcmc(as.numeric(beta.samples)), prob=0.95)
beta.hpd$`99` = HPDinterval(as.mcmc(as.numeric(beta.samples)), prob=0.99)
y.pred.hpd = list("90"=NULL, "95"=NULL, "99"=NULL)
y.pred.hpd$`90` = apply(y.pred.samples, 2, function(y) HPDinterval(as.mcmc(as.numeric(y)), prob=0.90))
y.pred.hpd$`95` = apply(y.pred.samples, 2, function(y) HPDinterval(as.mcmc(as.numeric(y)), prob=0.95))
y.pred.hpd$`99` = apply(y.pred.samples, 2, function(y) HPDinterval(as.mcmc(as.numeric(y)), prob=0.99))
y.pred.median = apply(y.pred.samples, 2, median)
mu.hpd95 = list(upper=NULL, lower=NULL)
for (j in 1:reg.model.data$M) {
    hpd = HPDinterval(as.mcmc(as.numeric(mu.cred.samples[, j])), prob=0.95)
    mu.hpd95$upper = c(mu.hpd95$upper, hpd[, "upper"])
    mu.hpd95$lower = c(mu.hpd95$lower, hpd[, "lower"])
}

# Print statistics
cat("\nWriting correlation and regression results to output directory...\n")
cat("SIMPLE LOGARITHMIC REGRESSION: POSTERIOR STATISTICS\n",
    "Intercept (alpha)\n",
    "   Posterior mean and standard deviation: Mean = ", 
    mean(alpha.samples), ", SD = ", sd(alpha.samples), "\n",
    "   Posterior median and MAD:              Median = ", 
    median(alpha.samples), ", MAD = ", mad(alpha.samples), "\n",
    "   Values with 95% posterior probability: 95% HPDI = [", 
    alpha.hpd$`95`[,"lower"], ", ", alpha.hpd$`95`[,"upper"], "]\n",
    "   Values with 99% posterior probability: 99% HPDI = [", 
    alpha.hpd$`99`[,"lower"], ", ", alpha.hpd$`99`[,"upper"], "]\n",
    "Slope (beta)\n",
    "   Posterior mean and standard deviation: Mean = ",
    mean(beta.samples), ", SD = ", sd(beta.samples), "\n",
    "   Posterior median and MAD:              Median = ", 
    median(beta.samples), ", MAD = ", mad(beta.samples), "\n",
    "   Values with 95% posterior probability: 95% HPDI = [", 
    beta.hpd$`95`[,"lower"], ", ", beta.hpd$`95`[,"upper"], "]\n",
    "   Values with 99% posterior probability: 99% HPDI = [", 
    beta.hpd$`99`[,"lower"], ", ", beta.hpd$`99`[,"upper"], "]\n",
    sep="", file=OUTPUT$LAT.SUMMARY, append=TRUE)
for (j in 1:reg.model.data$P) {
    cat("Predicted value #", j, " (x = ", reg.model.data$x_pred[j], ")\n",
        "   Posterior median predicted response:   Median = ", y.pred.median[j], "\n",
        "   Values with 90% posterior probability: 90% HPDI = [", 
        y.pred.hpd$`90`[1, j], ", ", y.pred.hpd$`90`[2, j], "]\n",
        "   Values with 95% posterior probability: 95% HPDI = [", 
        y.pred.hpd$`95`[1, j], ", ", y.pred.hpd$`95`[2, j], "]\n",
        "   Values with 99% posterior probability: 99% HPDI = [", 
        y.pred.hpd$`99`[1, j], ", ", y.pred.hpd$`99`[2, j], "]\n",
        sep="", file=OUTPUT$LAT.SUMMARY, append=TRUE)
}

# Plot regression
cairo_pdf(OUTPUT$LAT.REGRESSION, width=9, height=7)
par(mar=c(4, 4, 1, 1), mgp=c(3, 0.8, 0))
plot(reg.model.data$x, reg.model.data$y, type="n", las=1, cex.axis=0.8, yaxt="n", xaxt="n", 
     xlim=c(0, 0.39), ylim=c(-4, 57), xlab="", ylab="")
axis(side=2, at=seq(0,50,10), cex.axis=1.2, las=1)
axis(side=1, cex.axis=1.2)
mtext("Normalised CC>TT dinucleotide variants", side=1, line=2.5, cex=1.3)
mtext("Absolute mean latitude", side=2, line=2.5, cex=1.3)
polygon(x=c(reg.model.data$x_cred, rev(reg.model.data$x_cred)), 
        y=c(mu.hpd95$lower, rev(mu.hpd95$upper)), border=NA, col="#e5e5e5")
lines(x=reg.model.data$x_cred, y=colMeans(mu.cred.samples), lwd=3, col="#4d4d4d")
points(reg.model.data$x, reg.model.data$y, pch=16, col="#1874cd", cex=1.4)
points(x=reg.model.data$x_pred, y=y.pred.median, pch=16, col="#ee7600", cex=1.6)
segments(x0=reg.model.data$x_pred, y0=y.pred.hpd$`90`[1,], 
         x1=reg.model.data$x_pred, y1=y.pred.hpd$`90`[2,], 
         col="#ee7600", lwd=3.5)
mtext(gsub("-", "–", paste0("Spearman's ρ = ", round(median(rho.samples), 2))),
      side=3, line=-4, at=0.28, cex=1.5)
mtext(gsub("-", "–",
           paste0("95% HPDI = [", round(rho.hpd95[1], 2), ", ", round(rho.hpd95[2], 2), "]")),
      side=3, line=-6, at=0.28, cex=1.5)
invisible(dev.off())


# Output group data table
cat("Writing phylogenetic group information table to output directory...\n")
write.table(group.data[, c(16:23, 1:15)], file=OUTPUT$GROUP.DATA, sep="\t", quote=F, row.names=F)

cat("\nDone\n\n")

